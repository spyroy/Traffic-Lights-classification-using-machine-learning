# -*- coding: utf-8 -*-
"""project_imbalanced_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-HKQX7M_vLVKHOR7PM2ZaOLcwEZfW_Lt

#imports
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
import matplotlib.pyplot as plt
import seaborn as sns
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout 
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from sklearn.metrics import classification_report,confusion_matrix
import tensorflow as tf
import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import mean_squared_error

"""#Imbalanced Data

#generate data
"""

labels = ['pedestrian', 'regular']
img_size = 128

def get_data(data_dir):
    data = [] 
    for label in labels: 
        path = os.path.join(data_dir, label)
        class_num = labels.index(label)
        for img in os.listdir(path):
            try:
                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format
                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size
                data.append([resized_arr, class_num])
            except Exception as e:
                print(e)
    return np.array(data)

data = get_data('/content/drive/My Drive/machine learning/data')

"""#split to train and test"""

train,test = train_test_split(data, test_size=0.33, random_state=42)
print(train.shape)
print(test.shape)

"""#show the balance in the train"""

l = []
for i in train:
    if(i[1] == 0):
        l.append("pedestrian")
    else:
        l.append("regular")
sns.set_style('darkgrid')
sns.countplot(l)

"""#show the balance in the test"""

r = []
for i in test:
    if(i[1] == 0):
        r.append("pedestrian")
    else:
        r.append("regular")
sns.set_style('darkgrid')
sns.countplot(r)

"""#dislpay some images from the data"""

plt.figure(figsize = (5,5))
plt.imshow(train[1][0])
plt.title(labels[train[0][1]])

plt.figure(figsize = (5,5))
plt.imshow(train[-1][0])
plt.title(labels[train[-1][1]])

"""#preper the data"""

x_train = []
y_train = []
x_test = []
y_test = []

for feature, label in train:
  x_train.append(feature)
  y_train.append(label)

for feature, label in test:
  x_test.append(feature)
  y_test.append(label)

# Normalize the data
x_train = np.array(x_train) / 255
x_test = np.array(x_test) / 255

x_train.reshape(-1, img_size, img_size, 1)
y_train = np.array(y_train)

x_test.reshape(-1, img_size, img_size, 1)
y_test = np.array(y_test)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

"""#do some random things on the data for better results"""

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.2, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip = True,  # randomly flip images
        vertical_flip=False)  # randomly flip images


datagen.fit(x_train)

"""#CNN model"""

model = Sequential()
model.add(Conv2D(32,3,padding="same", activation="relu", input_shape=(img_size,img_size,3)))
model.add(MaxPool2D())

model.add(Conv2D(32, 3, padding="same", activation="relu"))
model.add(MaxPool2D())

model.add(Conv2D(64, 3, padding="same", activation="relu"))
model.add(MaxPool2D())
model.add(Dropout(0.6))

model.add(Flatten())
model.add(Dense(128,activation="relu"))
model.add(Dense(2, activation="softmax"))

model.summary()

opt = Adam(lr=0.0001)
model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])

epochs = 15
history = model.fit(x_train,y_train,epochs = epochs , validation_data = (x_test, y_test))

"""#show results of CNN"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

predictions = model.predict_classes(x_test)
predictions = predictions.reshape(1,-1)[0]
print(classification_report(y_test, predictions, target_names = ['pedestrain (Class 0)','regular (Class 1)']))

"""#KNN model"""

x_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]*x_train.shape[3])
x_train.shape

x_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3])
x_test.shape

knn_1 = KNeighborsClassifier(n_neighbors=1)
knn_1.fit(x_train,y_train)
knn_2 = KNeighborsClassifier(n_neighbors=2)
knn_2.fit(x_train,y_train)
knn_3 = KNeighborsClassifier(n_neighbors=3)
knn_3.fit(x_train,y_train)
knn_4 = KNeighborsClassifier(n_neighbors=4)
knn_4.fit(x_train,y_train)
knn_5 = KNeighborsClassifier(n_neighbors=5)
knn_5.fit(x_train,y_train)
# knn_9 = KNeighborsClassifier(n_neighbors=9)
# knn_9.fit(x_train,y_train)

"""#show results of KNN"""

score1 = knn_1.score(x_test, y_test)
score2 = knn_2.score(x_test, y_test)
score3 = knn_3.score(x_test, y_test)
score4 = knn_4.score(x_test, y_test)
score5 = knn_5.score(x_test, y_test)
print("1 neighbour: " ,score1)
print("2 neighbours: " ,score2)
print("3 neighbours: " ,score3)
print("4 neighbours: " ,score4)
print("5 neighbours: " ,score5)

plt.scatter(x_test[:,1],knn_1.predict_proba(x_test)[:,1])

"""#Logistic regression Model"""

logisticRegr = LogisticRegression()
result = logisticRegr.fit(x_train, y_train)

"""#show results of Logistic Regression"""

score = logisticRegr.score(x_test, y_test)
print(score)

plt.scatter(x_test[:,1],logisticRegr.predict_proba(x_test)[:,1])

"""#SVM model"""

svm = svm.SVC(kernel='linear',probability=True) # Linear Kernel
svm.fit(x_train, y_train)

"""#show results of SVM"""

y_pred = svm.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))

plt.scatter(x_test[:,1],svm.predict_proba(x_test)[:,1])

svm_preds = svm.predict(x_test)
knn_preds = knn_1.predict(x_test)
logistic_preds = logisticRegr.predict(x_test)

"""#Combine all models to see if it is improving results"""

# predictions
adaboost_preds = []

for x in range(len(predictions)):
  preds = []
  preds.append(svm_preds[x])
  preds.append(knn_preds[x])
  preds.append(logistic_preds[x])
  preds.append(predictions[x])
  adaboost_preds.append(max(set(preds), key = preds.count))

"""#results of the combined models"""

print("Accuracy:",metrics.accuracy_score(y_test, adaboost_preds))
print("Precision:",metrics.precision_score(y_test, adaboost_preds))
print("Recall:",metrics.recall_score(y_test, adaboost_preds))

target_names = ['pedestrian', 'regular']
print(classification_report(y_test, adaboost_preds, target_names=target_names))

print(mean_squared_error(y_test, adaboost_preds, multioutput='raw_values'))

print(confusion_matrix(y_test, adaboost_preds))

plt.scatter(x_test[:,1],adaboost_preds)

"""#Balanced Data

#Generate Data
"""

labels = ['pedestrian', 'regular']
img_size = 128

def get_data(data_dir):
    data = []
    for label in labels:
        path = os.path.join(data_dir, label)
        class_num = labels.index(label)
        for img in os.listdir(path):
            try:
                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format
                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size
                data.append([resized_arr, class_num])
            except Exception as e:
                print(e)
    return np.array(data)

data = get_data('/content/drive/My Drive/machine learning/balanced data')

"""#Split to train and test"""

train,test = train_test_split(data, test_size=0.33, random_state=42)
print(train.shape)
print(test.shape)

"""#Show the balance in the train"""

l = []
for i in train:
    if(i[1] == 0):
        l.append("pedestrian")
    else:
        l.append("regular")
sns.set_style('darkgrid')
sns.countplot(l)

"""#Show the balance in the test"""

r = []
for i in test:
    if(i[1] == 0):
        r.append("pedestrian")
    else:
        r.append("regular")
sns.set_style('darkgrid')
sns.countplot(r)

"""#Dislpay some images from the data"""

plt.figure(figsize = (5,5))
plt.imshow(train[1][0])
plt.title(labels[train[0][1]])

plt.figure(figsize = (5,5))
plt.imshow(train[-1][0])
plt.title(labels[train[-1][1]])

"""#Preper the data"""

x_train_list = []
y_train = []
x_test_list = []
y_test = []

for feature, label in train:
  x_train_list.append(feature)
  y_train.append(label)

for feature, label in test:
  x_test_list.append(feature)
  y_test.append(label)

# Normalize the data
x_train = np.array(x_train_list)/255
x_test = np.array(x_test_list)/255
# img = Image.fromarray(x_train[4])
# img.save('my.png')
# img.show()
x_train.reshape(-1, img_size, img_size, 1)
y_train = np.array(y_train)

x_test.reshape(-1, img_size, img_size, 1)
y_test = np.array(y_test)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

"""#Do some random things on the data for better results"""

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.2, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip = True,  # randomly flip images
        vertical_flip=False)  # randomly flip images


datagen.fit(x_train)

"""#CNN model"""

class CNN:

    def __init__(self, x_train, y_train, x_test, y_test, img_size):
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test
        self.img_size = img_size
        self.model = self.model()

    def model(self):
        model = Sequential()
        model.add(Conv2D(32, 3, padding="same", activation="relu", input_shape=(self.img_size, self.img_size, 3)))
        model.add(MaxPool2D())

        model.add(Conv2D(32, 3, padding="same", activation="relu"))
        model.add(MaxPool2D())

        model.add(Conv2D(64, 3, padding="same", activation="relu"))
        model.add(MaxPool2D())
        model.add(Dropout(0.6))

        model.add(Flatten())
        model.add(Dense(128, activation="relu"))
        model.add(Dense(2, activation="softmax"))

        model.summary()
        return model

    def run(self):

        opt = Adam(lr=0.0001)
        self.model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                    metrics=['accuracy'])

        epochs = 15
        history = self.model.fit(self.x_train, self.y_train, epochs=epochs, validation_data=(self.x_test, self.y_test))

        """#show results of CNN"""
        acc = history.history['accuracy']
        val_acc = history.history['val_accuracy']
        loss = history.history['loss']
        val_loss = history.history['val_loss']

        epochs_range = range(epochs)

        plt.figure(figsize=(15, 15))
        plt.subplot(2, 2, 1)
        plt.plot(epochs_range, acc, label='Training Accuracy')
        plt.plot(epochs_range, val_acc, label='Validation Accuracy')
        plt.legend(loc='lower right')
        plt.title('Training and Validation Accuracy')

        plt.subplot(2, 2, 2)
        plt.plot(epochs_range, loss, label='Training Loss')
        plt.plot(epochs_range, val_loss, label='Validation Loss')
        plt.legend(loc='upper right')
        plt.title('Training and Validation Loss')
        plt.show()

        predictions = self.model.predict_classes(x_test)
        predictions = predictions.reshape(1, -1)[0]
        print(classification_report(self.y_test, predictions, target_names=['pedestrain (Class 0)', 'regular (Class 1)']))
        return predictions

"""#KNN model"""

class KNN:

    def __init__(self, x_train, y_train, x_test, y_test):
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test

    def run(self):
        knn_1 = KNeighborsClassifier(n_neighbors=1)
        knn_1.fit(self.x_train, self.y_train)
        knn_2 = KNeighborsClassifier(n_neighbors=2)
        knn_2.fit(self.x_train, self.y_train)
        knn_3 = KNeighborsClassifier(n_neighbors=3)
        knn_3.fit(self.x_train, self.y_train)
        knn_4 = KNeighborsClassifier(n_neighbors=4)
        knn_4.fit(self.x_train, self.y_train)
        knn_5 = KNeighborsClassifier(n_neighbors=5)
        knn_5.fit(self.x_train, self.y_train)

        """#show results of KNN"""

        score1 = knn_1.score(self.x_test, self.y_test)
        score2 = knn_2.score(self.x_test, self.y_test)
        score3 = knn_3.score(self.x_test, self.y_test)
        score4 = knn_4.score(self.x_test, self.y_test)
        score5 = knn_5.score(self.x_test, self.y_test)
        print("1 neighbour: ", score1)
        print("2 neighbours: ", score2)
        print("3 neighbours: ", score3)
        print("4 neighbours: ", score4)
        print("5 neighbours: ", score5)

        plt.scatter(self.x_test[:, 1], knn_1.predict_proba(self.x_test)[:, 1])
        return knn_1

"""#Logistic Regression"""

class logisticRegression:

    def __init__(self, x_train, y_train, x_test, y_test):
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test

    def run(self):
        logisticRegr = LogisticRegression()
        result = logisticRegr.fit(self.x_train, self.y_train)

        """#show results of Logistic Regression"""

        score = logisticRegr.score(self.x_test, self.y_test)
        print(score)

        plt.scatter(self.x_test[:, 1], logisticRegr.predict_proba(self.x_test)[:, 1])
        return logisticRegr

"""#SVM model"""

class SVM:

    def __init__(self, x_train, y_train, x_test, y_test):
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test

    def run(self):
        SVM = svm.SVC(kernel='linear', probability=True)  # Linear Kernel
        SVM.fit(self.x_train, self.y_train)

        """#show results of SVM"""

        y_pred = SVM.predict(self.x_test)
        print("Accuracy:", metrics.accuracy_score(self.y_test, y_pred))
        print("Precision:", metrics.precision_score(self.y_test, y_pred))
        print("Recall:", metrics.recall_score(self.y_test, y_pred))

        plt.scatter(self.x_test[:, 1], SVM.predict_proba(self.x_test)[:, 1])
        return SVM

"""#Adaboost model (combining all models)"""

class adaboost:

    def __init__(self, x_test, y_test, cnn, knn, lgr, svm):
        self.x_test = x_test
        self.y_test = y_test
        self.cnn = cnn
        self.knn = knn
        self.lgr = lgr
        self.svm = svm

    def run(self):
        cnn_preds = self.cnn
        cnn_acc = metrics.accuracy_score(self.y_test, cnn_preds)
        svm_preds = self.svm.predict(self.x_test)
        svm_acc = metrics.accuracy_score(self.y_test, svm_preds)
        knn_preds = self.knn.predict(self.x_test)
        knn_acc = metrics.accuracy_score(self.y_test, knn_preds)
        logistic_preds = self.lgr.predict(self.x_test)
        logistic_acc = metrics.accuracy_score(self.y_test, logistic_preds)

        # predictions
        adaboost_preds = []

        for x in range(len(self.cnn)):
            preds = 0.
            if svm_preds[x] == 0:
                preds += svm_acc * -1
            else:
                preds += svm_acc * 1
            if knn_preds[x] == 0:
                preds += knn_acc * -1
            else:
                preds += knn_acc * 1
            if logistic_preds[x] == 0:
                preds += logistic_acc * -1
            else:
                preds += logistic_acc * 1
            if cnn_preds[x] == 0:
                preds += cnn_acc * -1
            else:
                preds += cnn_acc *1
            if preds >= 0:
                adaboost_preds.append(1)
            else:
                adaboost_preds.append(0)


        """#results of the combined models"""

        print("Accuracy:", metrics.accuracy_score(self.y_test, adaboost_preds))
        print("Precision:", metrics.precision_score(self.y_test, adaboost_preds))
        print("Recall:", metrics.recall_score(self.y_test, adaboost_preds))

        target_names = ['pedestrian', 'regular']
        print(classification_report(self.y_test, adaboost_preds, target_names=target_names))

        print(mean_squared_error(self.y_test, adaboost_preds, multioutput='raw_values'))

        print(confusion_matrix(self.y_test, adaboost_preds))

        plt.scatter(x_test[:, 1], adaboost_preds)

"""#Find Errors"""

def findErrors(x_test, y_test, algo,isCnn= False):
    lst = []
    if(isCnn):
        for idx, prediction, label in zip(enumerate(x_test), algo, y_test):
            if prediction != label:
                lst.append(idx[0])
    else:
        for idx, prediction, label in zip(enumerate(x_test), algo.predict(x_test), y_test):
            if prediction != label:
                lst.append(idx[0])
    # npArr = np.array(lst, dtype=int)
    print("list: ", lst)
    return np.array(lst, dtype=int)
    # np.save = ('svm.npy', npArr)

"""#Run models

#CNN Results
"""

img_size = 128

print("\n CNN \n")
CNN = CNN(x_train, y_train, x_test, y_test, img_size).run()

"""#KNN Results"""

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2] * x_train.shape[3])
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2] * x_test.shape[3])

print("\n KNN \n")
KNN = KNN(x_train, y_train, x_test, y_test).run()

"""#Logistic Regression Results"""

print("\n Logistic Regression \n")
LGR = logisticRegression(x_train, y_train, x_test, y_test).run()

"""#SVM Results"""

print("\n SVM \n")
SVM = SVM(x_train, y_train, x_test, y_test).run()

"""#Adaboost Results"""

print("\n Adaboost \n")
adb = adaboost(x_test, y_test, CNN, KNN, LGR, SVM)
adb.run()

new_data = get_data('/content/drive/My Drive/machine learning/new_data')

x = []
y = []

for feature, label in new_data:
  x.append(feature)
  y.append(label)

# Normalize the data
x = np.array(x) / 255

x.reshape(-1, img_size, img_size, 1)
y = np.array(y)

cnn_preds = model.predict_classes(x)
cnn_acc = metrics.accuracy_score(y, cnn_preds)
print(cnn_preds)
print(cnn_acc)

x_ = x.reshape(x.shape[0],x.shape[1]*x.shape[2]*x.shape[3])
x_.shape

svm_preds = SVM.predict(x_)
svm_acc = metrics.accuracy_score(y, svm_preds)
knn_preds = KNN.predict(x_)
knn_acc = metrics.accuracy_score(y, knn_preds)
logistic_preds = LGR.predict(x_)
logistic_acc = metrics.accuracy_score(y, logistic_preds)

print("SVM" , svm_acc)
print("KNN" ,knn_acc)
print("Logistic Regression" ,logistic_acc)

adb = adaboost(x_, y, cnn_preds, KNN, LGR, SVM)
adb.run()